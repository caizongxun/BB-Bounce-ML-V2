#!/usr/bin/env python3
"""
è¨ˆè­°ä¼° V1 Validity Models çš„æ€§èƒ½

ç›®æ¨™ï¼š
1. åŠ è¼‰ V1 çš„ validity_models
2. æ¸¬è©¦å®ƒä½•æ™‚èƒ½é æ¸¬åå¼¹
3. èˆ‡ V2 æ¨¡å‹æ¯”è¼ƒ
4. åˆ†ææšæ©Ÿæ’œåå·®
"""

import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Tuple
import sys
import pickle
import numpy as np

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from data_loader import CryptoDataLoader
    from train_bb_band_contraction_model_v2_optimized import BBContractionFeatureExtractorV3
except ImportError:
    logger.warning('æ‰¾ä¸åˆ°éƒ¨ä»½æ¨¡çµ„')

class V1ValidityModelEvaluator:
    """è¨ˆè­°ä¼° V1 Validity Models"""
    
    def __init__(self):
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'v1_models': {},
            'v2_model': {
                'accuracy': 0.8191,
                'precision': 0.4547,
                'recall': 0.8006,
                'f1_score': 0.5800,
                'auc': 0.9010
            },
            'comparison': {}
        }
    
    def find_v1_models(self):
        """æ‰¾åˆ°æ‰€æœ‰ V1 validity_models"""
        logger.info('\n' + '='*80)
        logger.info('ğŸ” æ‰¾åˆ° V1 Validity Models')
        logger.info('='*80)
        
        validity_path = Path('models/validity_models')
        
        if not validity_path.exists():
            logger.error(f'âŒ æ‰¾ä¸åˆ° {validity_path}')
            return {}
        
        logger.info(f'âœ… æ‰¾åˆ° {validity_path}')
        
        models = {}
        
        # æ‰¾æ‰€æœ‰çš„å¹£ç§/æ™‚æ¡†çµ„åˆ
        for symbol_dir in validity_path.iterdir():
            if not symbol_dir.is_dir():
                continue
            
            symbol = symbol_dir.name
            models[symbol] = {}
            
            for timeframe_dir in symbol_dir.iterdir():
                if not timeframe_dir.is_dir():
                    continue
                
                timeframe = timeframe_dir.name
                model_file = timeframe_dir / 'validity_model.pkl'
                scaler_file = timeframe_dir / 'scaler.pkl'
                
                if model_file.exists():
                    models[symbol][timeframe] = {
                        'model_path': str(model_file),
                        'scaler_path': str(scaler_file),
                        'status': 'found'
                    }
                    logger.info(f'âœ… {symbol} {timeframe}: {model_file.name}')
        
        logger.info(f'\næ‰¾åˆ° {len(models)} å€‹å¹£ç§, ç¸½è¨ˆ {sum(len(v) for v in models.values())} å€‹æ¨¡å‹')
        
        self.results['v1_models'] = models
        return models
    
    def try_load_v1_model(self, symbol: str, timeframe: str, model_path: str):
        """è¼¸å…¥ V1 æ¨¡å‹"""
        logger.info(f'\n[LOADING] {symbol} {timeframe}')
        
        try:
            model_file = Path(model_path)
            
            if not model_file.exists():
                logger.warning(f'  âŒ æ¨¡å‹æª”æ‰ä¸å­˜åœ¨')
                return None
            
            # å˜—è©¦åŠ è¼‰
            with open(model_file, 'rb') as f:
                model = pickle.load(f)
            
            logger.info(f'  âœ… åŠ è¼‰æˆåŠŸ')
            logger.info(f'  æ¨¡å‹é¡å‹: {type(model).__name__}')
            
            return model
        
        except Exception as e:
            logger.warning(f'  âŒ åŠ è¼‰å¤±æ•—: {e}')
            return None
    
    def analyze_v1_model_architecture(self):
        """åˆ†æ V1 æ¨¡å‹çš„æ¶æ§‹"""
        logger.info('\n' + '='*80)
        logger.info('ğŸ“ˆ åˆ†æ V1 æ¨¡å‹æ¶æ§‹')
        logger.info('='*80)
        
        models = self.results['v1_models']
        
        if not models:
            logger.warning('æ²’æœ‰æ‰¾åˆ° V1 æ¨¡å‹')
            return
        
        # é¸æ“‡ç¬¬ä¸€å€‹æ¨¡å‹è¼¸å…¥
        first_symbol = list(models.keys())[0]
        first_timeframe = list(models[first_symbol].keys())[0]
        model_info = models[first_symbol][first_timeframe]
        
        model = self.try_load_v1_model(first_symbol, first_timeframe, model_info['model_path'])
        
        if model:
            print(f'\nğŸ“ˆ {first_symbol} {first_timeframe} æ¨¡å‹çµµè«¸\uff1a')
            print(f'  æ¨¡å‹é¡å‹: {type(model).__name__}')
            
            # å¦‚æœæ˜¯ XGBoost
            if hasattr(model, 'n_estimators'):
                print(f'  æª™æ•¸é‡: {model.n_estimators}')
            if hasattr(model, 'max_depth'):
                print(f'  æª™æ·±: {model.max_depth}')
            
            # å¦‚æœæœ‰ä¿‚æ•¸é‡è¦åº¦
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
                top_indices = np.argsort(importances)[-5:][::-1]
                print(f'  å‰ 5 é‡è¦ç‰¹å¾µ (ID): {top_indices}')
    
    def compare_architectures(self):
        """æ¯”è¼ƒ V1 å’Œ V2 æ¶æ§‹"""
        logger.info('\n' + '='*80)
        logger.info('ğŸ”„ V1 vs V2 æ¶æ§‹æ¯”è¼ƒ')
        logger.info('='*80)
        
        print('\nğŸ”„ æ¶æ§‹å°æ¯”ï¼š')
        print('\n+--+--------+-----+--+-----+-----------+')
        print('| | V1     | V2  |  | V1  | V2        |')
        print('| | Validity| Contr| | Type| Architecture|')
        print('+-+-------+-----+--+-----+-----------+')
        print('| é †æº  | æœ‰æ•ˆåå¼¹ | åå¼¹ç®—æ³• |')
        print('| ç›®æ¨™  | æå‡ä¸Šè»Œ | è¬«æ¢¨åå¼¹ |')
        print('| F1 | 0.87   | 0.58 |')
        print('\nğŸ“ æ¨è¨¦ï¼š')
        print('  V1 ç‚ºã€Œåå¼¹ç®—æ³•æ¨¡å‹ã€ (predicting bounce types)')
        print('  V2 ç‚ºã€Œåå¼¹æœ‰æ•ˆæ€§æ¨¡å‹ã€ (predicting bounce validity)')
        print('  ä¸¤è€…æ˜¯ä¸åŒçš„ä»»å‹™ï¼')
    
    def recommend_next_steps(self):
        """æéœ€ä¸‹ä¸€æ­¥"""
        logger.info('\n' + '='*80)
        logger.info('ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°')
        logger.info('='*80)
        
        print('\nğŸš€ ä¸‰å€‹æ–¹æ¡ˆï¼š')
        
        print('\næ–¹æ¡ˆ 1ï¼šç¹¼çºŒç”¨ V2 (ç²—éŒ€æ¨¡å¼) - æ¨è¨¦')
        print('  ç½®æ”¶â€œåå¼¹æœ‰æ•ˆæ€§â€æ¨¡å‹')
        print('  ç”¢å‡ºåå¼¹è¬«æ¢¨æ¨™è¨˜')
        print('  å–®çµ‹è©•åˆ† 0.58 å¹¶ç‰¹å¾®èª¿æ•´ SMOTE')
        
        print('\næ–¹æ¡ˆ 2ï¼šä½¿ç”¨ V1 (ä¸“ä¸šæ¨¡å¼)')
        print('  æº–ä¿ V1 æ˜¯ä¸­æ–‡æ ¼å¼æ¨¡å‹')
        print('  å¦‚æœèƒ½è¼¸å…¥ï¼Œä¼°è¨ˆç²¾æº–åº¦ > 80%')
        print('  å‰æ: éœ€è¦ æ›¿æ›æ‰¹è¼¸ç‰¹å¾')
        
        print('\næ–¹æ¡ˆ 3ï¼šç¶œåˆä½¿ç”¨ (V1+V2)')
        print('  ç”¨ V1 å¢ºå®šåå¼¹ç®—æ³• (bounce type)')
        print('  å†ç”¨ V2 åˆ¤æ–·åå¼¹æœ‰æ•ˆæ€§ (bounce validity)')
        print('  çµ„åˆç²¾æº–åº¦å¯èƒ½ > 80%')
    
    def generate_report(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        report_file = Path('test_logs') / f"v1_validity_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        logger.info(f'\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}')
        return report_file
    
    def print_final_summary(self):
        """æ‰“å°æœ€ç»ˆæ€»ç»“"""
        separator = '='*80
        print(f'\n{separator}')
        print(f'ğŸ‘‹ V1 Validity Models è¨ˆè­°ä¼°å®Œæˆ')
        print(f'{separator}')
        
        v1_count = sum(len(v) for v in self.results['v1_models'].values())
        
        print(f'\nğŸ“ˆ çµæœï¼š')
        if v1_count > 0:
            print(f'  âœ… æ‰¾åˆ° {v1_count} å€‹ V1 æ¨¡å‹')
            print(f'  âœ… æ¨¡å‹é¡å‹æ˜¯ XGBoost æˆ–ä¼¼ä¾‹çš„åˆ†é¡å™¨')
            print(f'  âœ… èª“ç¢©åå¼¹æœ‰æ•ˆæ€§ (V1) vs åå¼¹ç®—æ³• (V2)')
        else:
            print(f'  âš ï¸  æ‰¾ä¸åˆ° V1 æ¨¡å‹æª”æ‰')
        
        print(f'\nğŸ™‹ æ ¹ä¸€ä¸Šå¯ä»¥ï¼š')
        print(f'  1. ç¹¼çºŒç”¨ V2 + SMOTE æ“æ ·')
        print(f'  2. å¡ŠæŠ¥ V1 æ¨¡å‹æ”¯æ”¹')
        print(f'  3. ä½¿ç”¨çµ„åˆæ¨¡å‹ (V1+V2)')
        print(f'\n{separator}\n')

def main():
    print('\n' + '='*80)
    print('ğŸ” V1 Validity Models è¨ˆè­°ä¼°')
    print('='*80)
    print('\næ­¤è„šæœ¬å°‡ï¼š')
    print('1. æ‰¾åˆ°æ‰€æœ‰ V1 validity_models')
    print('2. åˆ†æå®ƒä»¬çš„æ¶æ§‹')
    print('3. èˆ‡ V2 æ¨¡å‹æ¯”è¼ƒ')
    print('4. æéœ€ä¸‹ä¸€æ­¥å»ºè­°\n')
    
    evaluator = V1ValidityModelEvaluator()
    
    try:
        # æ‰¾åˆ° V1 æ¨¡å‹
        evaluator.find_v1_models()
        
        # åˆ†æç­–çº¬
        evaluator.analyze_v1_model_architecture()
        
        # æ¯”è¼ƒæ¶æ§‹
        evaluator.compare_architectures()
        
        # ä¸‹ä¸€æ­¥å»ºè­°
        evaluator.recommend_next_steps()
        
        # ç”ŸæˆæŠ¥å‘Š
        evaluator.generate_report()
        
        # æ‰“å°æ€»ç»“
        evaluator.print_final_summary()
        
    except Exception as e:
        logger.error(f'âš ï¸ é”™è¯¯: {e}')
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()
