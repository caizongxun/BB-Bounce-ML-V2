#!/usr/bin/env python3
"""
è¶…åƒæ•°èª¿æ•´å™¨ - å®¢è§€å…¬å¼ç‰ˆæœ¬

ä¸ºæ¯ä¸ªå¸ç§æ–°æ‰¾æœ€ä¼˜çš„ XGBoost è¶…åƒæ•°
ä½¿ç”¨ Grid Search ä¸æ˜¯ Optunaï¼ˆæ—¨åœ¨æ§åˆ¶è€—æ™‚ï¼‰
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score
from xgboost import XGBClassifier
import warnings
import json

warnings.filterwarnings("ignore")

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

try:
    from data_loader import CryptoDataLoader
except ImportError:
    logger.error("æ‰¾ä¸åˆ° data_loader")
    exit(1)

from train_bb_band_v2_objective_formula import BBContractionFeatureExtractorV3_Objective


class HyperparameterTunerV2_Objective:
    """è¶…åƒæ•°èª¿æ•´å™¨ - å®¢è§€å…¬å¼ç‰ˆæœ¬"""

    def __init__(self, output_dir="hyperparameter_tuning_v2_objective"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.loader = CryptoDataLoader()

    def prepare_data(self, symbol: str, timeframe: str):
        """æ¸†æ•´è¨“ç·´æ•¸æ“š"""
        logger.info(f"ğŸ”§ æ¸†æ•´ {symbol} {timeframe} æ•¸æ“š...")

        # ä¸‹è¼‰æ•°æ®
        df = self.loader.download_symbol_data(symbol, timeframe)
        if df is None or len(df) < 100:
            logger.error(f"{symbol} {timeframe} æ•¸æ“šä¸è¶³")
            return None, None, None, None

        # æå–ç‰¹å¾ï¼ˆä½¿ç”¨å®¢è§€å…¬å¼ï¼‰
        extractor = BBContractionFeatureExtractorV3_Objective()
        df = extractor.create_features(df, timeframe=timeframe, lookahead=5)

        # ç¯©é¸æœ‰æ•ˆæ¨™ç±¤
        df_labeled = df[df["label_bounce_valid"] != -1].copy()

        if len(df_labeled) < 50:
            logger.error(f"{symbol} {timeframe} æœ‰æ•ˆæ¨£æœ¬ä¸è¶³")
            return None, None, None, None

        # ç‰¹å¾é¸æ“‡
        feature_cols = extractor.features
        X = df_labeled[feature_cols]
        y = df_labeled["label_bounce_valid"]

        # ç§»é™¤ NaN
        mask = ~X.isna().any(axis=1)
        X = X[mask]
        y = y[mask]

        # æ›¿æ›ç„¡çª®å¤§
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.median())
        X = X.fillna(0)

        if len(X) < 30:
            logger.error(f"{symbol} {timeframe} æ•¸æ“šä¸è¶³")
            return None, None, None, None

        logger.info(f"âœ… æœ‰æ•ˆæ¨£æœ¬: {len(X):,}")

        return X, y, feature_cols, df

    def tune_grid_search(self, symbol: str, timeframe: str):
        """ä½¿ç”¨ Grid Search é€²è¡Œè¶…åƒæ•°èª¿æ•´"""
        logger.info(f"ğŸ’ Grid Search æ£§é”è¶…åƒæ•°...")

        # æ¸†æ•´æ•¸æ®
        X, y, feature_cols, df = self.prepare_data(symbol, timeframe)
        if X is None:
            return None, None

        # åˆ†å‰²æ•°æ®
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
        except:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # å®šç¾©ç¶²æ ¼
        param_grid = {
            "n_estimators": [100, 150, 200, 250],
            "max_depth": [5, 6, 7, 8, 9],
            "learning_rate": [0.03, 0.05, 0.08, 0.1],
            "reg_alpha": [0.5, 1.0, 2.0],
            "reg_lambda": [1.0, 2.0, 3.0],
        }

        best_score = -1
        best_params = None
        trial_count = 0

        # ç®€åŒ–ç¶²æ ¼ï¼ˆåªæµ‹è¯•æœ€é‡è¦çš„åƒæ•°ï¼‰
        logger.info("ç®€åŒ– Grid Searchï¼ˆæ¸¬è¯•å…³é”®åƒæ•°ï¼‰...")

        simplified_grid = {
            "n_estimators": [100, 200, 250],
            "max_depth": [6, 7, 8],
            "learning_rate": [0.05, 0.08],
            "reg_alpha": [0.5, 1.0],
            "reg_lambda": [1.0, 2.0],
        }

        from itertools import product

        for params_tuple in product(*simplified_grid.values()):
            trial_count += 1
            params = dict(zip(simplified_grid.keys(), params_tuple))

            # è¨­ç½®é»˜è®¤å€¼
            params["subsample"] = 0.8
            params["colsample_bytree"] = 0.7

            model = XGBClassifier(
                **params,
                random_state=42,
                eval_metric="logloss",
                verbosity=0,
                scale_pos_weight=len(y_train[y_train == 0])
                / (len(y_train[y_train == 1]) + 1e-8),
            )

            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)

            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)
            score = 0.7 * precision + 0.3 * recall

            logger.info(
                f"[{trial_count}] n_est={params['n_estimators']}, depth={params['max_depth']}, lr={params['learning_rate']:.2f} => åˆ†æ•¸={score:.4f}"
            )

            if score > best_score:
                best_score = score
                best_params = params.copy()

        logger.info(f"ğŸ” æœ€ä¼˜è¶…åƒæ•°: {best_params}")
        logger.info(f"ğŸ” æœ€ä¼˜åˆ†æ•°: {best_score:.4f}")

        return best_params, best_score

    def save_best_params(self, symbol: str, timeframe: str, params: dict, score: float):
        """ä¿å­˜æœ€ä¼˜è¶…åƒæ•°"""
        results_file = (
            self.output_dir / f"{symbol}_{timeframe}_best_params.json"
        )

        data = {
            "symbol": symbol,
            "timeframe": timeframe,
            "best_params": params,
            "best_score": score,
            "timestamp": datetime.now().isoformat(),
        }

        with open(results_file, "w") as f:
            json.dump(data, f, indent=2)

        logger.info(f"âœ… ç»“æœä¿å­˜è‡³: {results_file}")
        return results_file

    def run_tuning(self, symbols=None, timeframes=None):
        """è¿è¡Œè¶…åƒæ•°èª¿æ•´"""
        if symbols is None:
            symbols = self.loader.symbols
        if timeframes is None:
            timeframes = self.loader.timeframes

        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ é–‹å§‹è¶…åƒæ•°èª¿æ•´ (å®¢è§€å…¬å¼ç‰ˆæœ¬)")
        logger.info(f"{'='*80}")
        logger.info(f"å¹£ç§: {len(symbols)}, æ™‚æ¡†: {len(timeframes)}")
        logger.info(f"ç¸½ä»»å‹™: {len(symbols) * len(timeframes)}")

        for symbol in symbols:
            for timeframe in timeframes:
                logger.info(f"\nâ¬‡ï¸ èª¿æ•´ {symbol} {timeframe}...")

                try:
                    best_params, best_score = self.tune_grid_search(
                        symbol, timeframe
                    )

                    if best_params:
                        self.save_best_params(symbol, timeframe, best_params, best_score)
                    else:
                        logger.warning(f"{symbol} {timeframe} èª¿æ•´å¤±æ•—")

                except Exception as e:
                    logger.error(f"{symbol} {timeframe} é”™è¯¯: {e}")
                    import traceback

                    traceback.print_exc()

        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… è¶…åƒæ•°èª¿æ•´å®Œæˆ")
        logger.info(f"{'='*80}")
        logger.info(f"ç»“æœä¿å­˜äº: {self.output_dir}")


if __name__ == "__main__":
    import sys

    tuner = HyperparameterTunerV2_Objective()

    if len(sys.argv) > 1 and sys.argv[1] == "--full":
        # æ‰€æœ‰å¹£ç§
        tuner.run_tuning()
    else:
        # å¿«é€Ÿæ¨¡å¼ï¼šä»… BTC/ETH
        tuner.run_tuning(symbols=["BTCUSDT", "ETHUSDT"], timeframes=["15m", "1h"])
