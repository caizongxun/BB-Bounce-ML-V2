#!/usr/bin/env python3
"""
è‡ªå‹•è¶…åƒæ•°èª¿æ•´å™¨

æ¾„æ´…è¾›è‹¦çš„ç½‘æ ¼æœç´¢ï¼Œç‚ºæ¯ä¸ªå¸ç§æ‰¾åˆ°æœ€ä¼˜çš„è¶…åƒæ•°é…ç½®

ä½¿ç”¨ Optuna æˆ–æ™®é€š grid search
"""

import pandas as pd
import numpy as np
from pathlib import Path
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import f1_score, precision_score, recall_score
from xgboost import XGBClassifier
import warnings
import logging
from datetime import datetime
import json

warnings.filterwarnings('ignore')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    logger.warning('Optuna æœªå®‰è£ï¼Œæ”¹ç”¨ Grid Search')

try:
    from data_loader import CryptoDataLoader
except ImportError:
    logger.error('æ‰¾ä¸åˆ° data_loaderï¼Œè«‹ç¢ºä¿åœ¨æ­£ç¢ºçš„ç›®éŒ„')
    exit(1)

from train_bb_band_contraction_model_v2_optimized import BBContractionFeatureExtractorV3

# ============================================================
# è¶…åƒæ•°èª¿æ•´å™¨
# ============================================================

class HyperparameterTuner:
    """ç‚ºæ¨¡å‹æ‰¾æœ€ä¼˜çš„è¶…åƒæ•°"""
    
    def __init__(self, output_dir='hyperparameter_tuning'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.loader = CryptoDataLoader()
        self.results = {}
    
    def prepare_data(self, symbol: str, timeframe: str):
        """æ¸†æ•¶è¨“ç·´æ•¸æ“š"""
        logger.info(f'æ¸†æ•¶ {symbol} {timeframe} æ•¸æ“š...')
        
        # ä¸‹è¼‰æ•°æ®
        df = self.loader.download_symbol_data(symbol, timeframe)
        if df is None or len(df) < 100:
            return None, None, None, None
        
        # æå–ç‰¹å¾
        extractor = BBContractionFeatureExtractorV3()
        df = extractor.create_features(df, timeframe=timeframe, lookahead=5)
        
        # ç¯©é¸æœ‰æ•ˆæ¨™ç±¤
        df_labeled = df[df['label_bounce_valid'] != -1].copy()
        
        if len(df_labeled) < 50:
            return None, None, None, None
        
        # ç‰¹å¾é¸æ“‡
        feature_cols = [
            'bb_width_change_1bar', 'bb_width_change_2bar', 'bb_width_change_3bar', 'bb_width_change_5bar',
            'bb_width_percentile', 'std_change', 'std_change_3bar',
            'bb_distance_change', 'bb_width_acceleration',
            'bb_width_trend', 'bb_squeeze_score',
            'rsi_14', 'price_bb_position', 'momentum_5', 'momentum_10', 'momentum_confluence',
            'volume_ratio', 'volume_strength', 'vol_ratio', 'historical_vol'
        ]
        
        X = df_labeled[feature_cols]
        y = df_labeled['label_bounce_valid']
        
        # ç§»é™¤ NaN
        mask = ~X.isna().any(axis=1)
        X = X[mask]
        y = y[mask]
        
        # ========================================
        # æ–°å¢ï¼šæ¸…ç†ç„¡ç©·å¤§å€¼å’Œ NaN
        # ========================================
        
        # æ›¿æ›ç„¡ç©·å¤§
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # æ›¿æ›è¶…å¤§å€¼ (> 1e10)
        X = X.clip(-1e10, 1e10)
        
        # è™•ç†äº®ç…§çš„ NaN
        X = X.fillna(X.median())
        
        # æœ€ä¾Œæƒ…æ³ï¼šå¦‚æœé‚„æœ‰ NaNï¼Œå°±ç”¨ 0 å¡«å……
        X = X.fillna(0)
        
        # æœƒæª¢æŸ¥æ˜¯å¦é‚„æœ‰ NaN æˆ–ä¸€æ—¥ç„¡ç©·å¤§
        if X.isna().any().any() or np.isinf(X).any().any():
            logger.warning(f'{symbol} {timeframe}: æ•¸æ“šä¸­äº«æœ‰ NaN æˆ–ä¸€æ—¥ç„¡ç©·å¤§ï¼Œè·³é')
            return None, None, None, None
        
        if len(X) < 30:
            return None, None, None, None
        
        logger.info(f'æœ‰æ•ˆæ¨£æœ¬: {len(X)}')
        
        return X, y, feature_cols, df
    
    def objective_optuna(self, trial, X_train, y_train, X_test, y_test):
        """ç”¨æ–¼ Optuna çš„ç›®æ¨™å‡½æ•¸"""
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 50, 300),
            'max_depth': trial.suggest_int('max_depth', 5, 12),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),
            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),
        }
        
        model = XGBClassifier(
            **params,
            random_state=42,
            eval_metric='logloss',
            verbosity=0,
            scale_pos_weight=len(y_train[y_train==0]) / (len(y_train[y_train==1]) + 1e-8)
        )
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        # ä¼˜åŒ–ç›®æ¨™ï¼šç²¾å‡†åº¦ + å¬å›ç‡
        # (äº¤æ˜“ä¸Šæœ€é‡è¦çš„æ˜¯ç²¾å‡†åº¦)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        
        # æƒé‡ï¼š70% ç²¾å‡†åº¦ + 30% å¬å›ç‡
        score = 0.7 * precision + 0.3 * recall
        
        return score
    
    def tune_optuna(self, symbol: str, timeframe: str, n_trials=50):
        """ä½¿ç”¨ Optuna é€²è¡Œè¶…åƒæ•°èª¿æ•´"""
        logger.info(f'ä½¿ç”¨ Optuna æª¢æŸ¥è¶…åƒæ•°...')
        
        # æ¸†æ•´æ•¸æ®
        X, y, feature_cols, df = self.prepare_data(symbol, timeframe)
        if X is None:
            logger.error(f'{symbol} {timeframe} æ•°æ®ä¸è¶³')
            return None
        
        # åˆ†å‰²æ•°æ®
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
        except:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
        
        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # æ‰€æœ‰å¹³å¾Œï¼ˆä¸ºäº† Optuna ä½¿ç”¨ï¼‰
        X_train = X_train_scaled
        X_test = X_test_scaled
        
        # åˆå§‹åŒ– Optuna study
        sampler = optuna.samplers.TPESampler(seed=42)
        study = optuna.create_study(sampler=sampler, direction='maximize')
        
        # å®šä¹‰ç›®æ¨™å‡½æ•°
        def objective(trial):
            return self.objective_optuna(trial, X_train, y_train, X_test, y_test)
        
        # é‹è¡Œç™¼ç¾
        logger.info(f'é‹è¡Œ {n_trials} æ¬¡è©¦é©—...')
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        # å–æœ€ä¼˜ç»“æœ
        best_trial = study.best_trial
        best_params = best_trial.params
        best_score = best_trial.value
        
        logger.info(f'æœ€ä¼˜è¶…åƒæ•°ï¼š{best_params}')
        logger.info(f'æœ€ä¼˜åˆ†æ•°ï¼š{best_score:.4f}')
        
        return best_params, best_score, study
    
    def tune_grid_search(self, symbol: str, timeframe: str):
        """ä½¿ç”¨ Grid Search é€²è¡Œè¶…åƒæ•°èª¿æ•´ (å½“ Optuna ä¸å¯ç”¨æ™‚)"""
        logger.info(f'ä½¿ç”¨ Grid Search æª¢æŸ¥è¶…åƒæ•°...')
        
        # æ¸†æ•´æ•°æ®
        X, y, feature_cols, df = self.prepare_data(symbol, timeframe)
        if X is None:
            logger.error(f'{symbol} {timeframe} æ•°æ®ä¸è¶³')
            return None
        
        # åˆ†å‰²æ•°æ®
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
        except:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
        
        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # å®šä¹‰ç½‘æ ¼
        param_grid = {
            'n_estimators': [100, 150, 200, 250],
            'max_depth': [5, 6, 7, 8, 9],
            'learning_rate': [0.03, 0.05, 0.08, 0.1],
            'subsample': [0.7, 0.8, 0.9],
            'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
            'reg_alpha': [0, 0.5, 1.0, 2.0],
            'reg_lambda': [1.0, 1.5, 2.0, 2.5],
        }
        
        best_score = -1
        best_params = None
        trial_count = 0
        
        # ç®€åŒ–ç½‘æ ¼ï¼ˆåªæµ‹è¯•æœ€é‡è¦çš„åƒæ•°ï¼‰
        logger.info('ç®€åŒ– Grid Searchï¼ˆåªæµ‹è¯•å…³é”®åƒæ•°ï¼‰...')
        
        simplified_grid = {
            'n_estimators': [100, 200, 250],
            'max_depth': [6, 7, 8],
            'learning_rate': [0.05, 0.08],
            'reg_alpha': [0.5, 1.0],
            'reg_lambda': [1.0, 2.0],
        }
        
        from itertools import product
        
        for params_tuple in product(*simplified_grid.values()):
            trial_count += 1
            params = dict(zip(simplified_grid.keys(), params_tuple))
            
            # è¨­ç½®é»˜è®¤å€¼
            params['subsample'] = 0.8
            params['colsample_bytree'] = 0.7
            
            model = XGBClassifier(
                **params,
                random_state=42,
                eval_metric='logloss',
                verbosity=0,
                scale_pos_weight=len(y_train[y_train==0]) / (len(y_train[y_train==1]) + 1e-8)
            )
            
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            
            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)
            score = 0.7 * precision + 0.3 * recall
            
            logger.info(f'[{trial_count}] n_est={params["n_estimators"]}, depth={params["max_depth"]}, lr={params["learning_rate"]:.2f} => åˆ†æ•°={score:.4f}')
            
            if score > best_score:
                best_score = score
                best_params = params.copy()
        
        logger.info(f'æœ€ä¼˜è¶…åƒæ•°ï¼š{best_params}')
        logger.info(f'æœ€ä¼˜åˆ†æ•°ï¼š{best_score:.4f}')
        
        return best_params, best_score, None
    
    def save_best_params(self, symbol: str, timeframe: str, params: dict, score: float):
        """ä¿å­˜æœ€ä¼˜è¶…åƒæ•°"""
        results_file = self.output_dir / f'{symbol}_{timeframe}_best_params.json'
        
        data = {
            'symbol': symbol,
            'timeframe': timeframe,
            'best_params': params,
            'best_score': score,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(results_file, 'w') as f:
            json.dump(data, f, indent=2)
        
        logger.info(f'ç»“æœä¿å­˜è‡³: {results_file}')
        
        return results_file
    
    def run_tuning(self, symbols=None, timeframes=None):
        """
        è¿è¡Œè¶…åƒæ•°èª¿æ•´
        """
        if symbols is None:
            symbols = self.loader.symbols
        if timeframes is None:
            timeframes = self.loader.timeframes
        
        logger.info(f'ğŸš€ é–‹å§‹è¶…åƒæ•°èª¿æ•´...')
        logger.info(f'æ–‡ä»¶ä¸‹è¼‰ä½ç½®: {self.output_dir}')
        
        for symbol in symbols:
            for timeframe in timeframes:
                logger.info(f'\nâ¬‡ï¸ èª¿æ•´ {symbol} {timeframe}...')
                
                try:
                    if OPTUNA_AVAILABLE:
                        best_params, best_score, study = self.tune_optuna(symbol, timeframe, n_trials=30)
                    else:
                        best_params, best_score, study = self.tune_grid_search(symbol, timeframe)
                    
                    if best_params:
                        self.save_best_params(symbol, timeframe, best_params, best_score)
                        self.results[f'{symbol}_{timeframe}'] = {
                            'params': best_params,
                            'score': best_score
                        }
                    else:
                        logger.warning(f'{symbol} {timeframe} èª¿æ•´å¤±æ•—')
                
                except Exception as e:
                    logger.error(f'{symbol} {timeframe} é”™è¯¯: {e}')
                    import traceback
                    traceback.print_exc()
        
        # æœ€ç»ˆç»Ÿè®¡
        logger.info(f'\n\u2705 è¶…åƒæ•°èª¿æ•´å®Œæˆï¼')
        logger.info(f'æˆåŠŸ: {len(self.results)}/{len(symbols) * len(timeframes)}')
        logger.info(f'ç»“æœä¿å­˜äº: {self.output_dir}')


if __name__ == '__main__':
    import sys
    
    tuner = HyperparameterTuner()
    
    # ä»…æµ‹è¯• BTC å’Œ ETH (å¿«é€Ÿæ¨¡å¼)
    symbols = ['BTCUSDT', 'ETHUSDT']
    timeframes = ['15m', '1h']
    
    if len(sys.argv) > 1 and sys.argv[1] == '--full':
        # å…¨éƒ¨å¹£ç§æ¨¡å¼
        logger.info('èŒ…ä½œ: å…¨éƒ¨å¹£ç§æ¨¡å¼')
        tuner.run_tuning()  # ä½¿ç”¨é»˜è®¤çš„æ‰€æœ‰å¹£ç§
    else:
        # å¿«é€Ÿæ¨¡å¼ï¼ˆä»… BTC/ETHï¼‰
        logger.info('èŒ…ä½œ: å¿«é€Ÿæ¨¡å¼ (BTC/ETH)')
        tuner.run_tuning(symbols=symbols, timeframes=timeframes)
