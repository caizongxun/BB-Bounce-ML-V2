import pandas as pd
import numpy as np
import joblib
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from data_loader import CryptoDataLoader
from label_generator import LabelGenerator

class PredictionAnalyzer:
    def __init__(self, models_dir='models'):
        self.models_dir = Path(models_dir)
        self.bb_models_dir = self.models_dir / 'bb_models'
        self.loader = CryptoDataLoader()
        self.generator = LabelGenerator(period=20, std_dev=2)
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å¾ K ç·šæ•¸æ“šè£½ä½œç‰¹å¾µ
        """
        df = df.copy()
        close_col = 'close' if 'close' in df.columns else 'Close'
        
        if 'open' not in df.columns and 'Open' in df.columns:
            df['open'] = df['Open']
            df['high'] = df['High']
            df['low'] = df['Low']
        
        # 1. åƒ¹æ ¼ä½ç½®ï¼ˆç›¸å°æ–¼ BB ä¸­è»¸ï¼‰
        df['price_to_bb_middle'] = (df[close_col] - df['bb_middle']) / df['bb_middle']
        
        # 2. åƒ¹æ ¼è·é›¢ä¸Š/ä¸‹è»Œ
        df['dist_upper_norm'] = (df['bb_upper'] - df[close_col]) / (df['bb_upper'] - df['bb_lower'])
        df['dist_lower_norm'] = (df[close_col] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        # 3. BB å¯¶å¯¬ï¼ˆBBW: Bollinger Bands Widthï¼‰
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
        
        # 4. RSI
        df = self.calculate_rsi(df)
        
        # 5. æ³¢å‹•æ€§
        df['volatility'] = df['volatility'].fillna(df['volatility'].mean())
        
        # 6. åƒ¹æ ¼å‹•é‡ï¼ˆæ—¥å¹¾ä½•å¹£ç‡ï¼‰
        df['returns'] = df[close_col].pct_change()
        df['returns_std'] = df['returns'].rolling(window=20).std()
        
        # 7. åƒ¹æ ¼èµ°å‹¢
        df['high_low_ratio'] = df['high'] / df['low'] - 1 if 'high' in df.columns else 0
        df['close_open_ratio'] = df[close_col] / df['open'] - 1 if 'open' in df.columns else 0
        
        # 8. ç§»å‹•å¹³å‡
        df['sma_5'] = df[close_col].rolling(window=5).mean()
        df['sma_20'] = df[close_col].rolling(window=20).mean()
        df['sma_50'] = df[close_col].rolling(window=50).mean()
        
        # æ¨£æœ¬æ•¸æ“šæ­£è­‰åŒ–
        df = df.ffill().bfill()
        
        return df
    
    def calculate_rsi(self, df: pd.DataFrame, period=14) -> pd.DataFrame:
        """è¨ˆç®— RSI (Relative Strength Index)"""
        df = df.copy()
        close_col = 'close' if 'close' in df.columns else 'Close'
        
        delta = df[close_col].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        df['rsi'] = df['rsi'].fillna(50)
        
        return df
    
    def analyze_symbol(self, symbol: str, timeframe: str):
        """
        åˆ†æç‰¹å®šå¹£ç¨®çš„é æ¸¬ä¿¡å¿ƒåº¦åˆ†å¸ƒ
        """
        print(f'\n{"="*60}')
        print(f'åˆ†æ {symbol} {timeframe} æ¨¡å‹')
        print(f'{"="*60}')
        
        try:
            # 1. åŠ è¼‰æ¨¡å‹
            model_path = self.bb_models_dir / symbol / timeframe / 'model.pkl'
            scaler_path = self.bb_models_dir / symbol / timeframe / 'scaler.pkl'
            
            if not model_path.exists():
                print(f'âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}')
                return
            
            model = joblib.load(model_path)
            scaler = joblib.load(scaler_path)
            
            print(f'âœ… å·²åŠ è¼‰æ¨¡å‹')
            
            # 2. ä¸‹è¼‰æ•¸æ“š
            df = self.loader.download_symbol_data(symbol, timeframe)
            if df is None:
                print(f'âŒ ä¸‹è¼‰å¤±æ•—')
                return
            
            print(f'âœ… ä¸‹è¼‰ {len(df)} æ ¹ K æ£’')
            
            # 3. ç”¢ç”Ÿæ¨™ç±¤
            df = self.generator.create_training_dataset(df, lookahead=5, touch_range=0.02)
            
            # 4. ç”¢ç”Ÿç‰¹å¾µ
            df = self.create_features(df)
            
            # 5. æº–å‚™ç‰¹å¾µ
            feature_cols = [
                'price_to_bb_middle', 'dist_upper_norm', 'dist_lower_norm',
                'bb_width', 'rsi', 'volatility', 'returns_std',
                'high_low_ratio', 'close_open_ratio',
                'sma_5', 'sma_20', 'sma_50'
            ]
            
            X = df[feature_cols].ffill().bfill()
            
            # 6. é€²è¡Œé æ¸¬ï¼ˆæ¦‚ç‡ï¼‰
            X_scaled = scaler.transform(X)
            probabilities = model.predict_proba(X_scaled)
            
            # 7. åˆ†æä¿¡å¿ƒåº¦åˆ†å¸ƒ
            print(f'\nğŸ“Š ä¿¡å¿ƒåº¦åˆ†å¸ƒåˆ†æï¼š')
            print(f'  ç¸½é æ¸¬æ•¸: {len(probabilities)}')
            
            # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æœ€å¤§æ¦‚ç‡ï¼ˆä¿¡å¿ƒåº¦ï¼‰
            confidences = np.max(probabilities, axis=1)
            
            print(f'\nğŸ” ä¿¡å¿ƒåº¦çµ±è¨ˆï¼š')
            print(f'  æœ€å°ä¿¡å¿ƒåº¦: {np.min(confidences):.4f} ({np.min(confidences)*100:.2f}%)')
            print(f'  æœ€å¤§ä¿¡å¿ƒåº¦: {np.max(confidences):.4f} ({np.max(confidences)*100:.2f}%)')
            print(f'  å¹³å‡ä¿¡å¿ƒåº¦: {np.mean(confidences):.4f} ({np.mean(confidences)*100:.2f}%)')
            print(f'  ä¸­ä½æ•¸ä¿¡å¿ƒåº¦: {np.median(confidences):.4f} ({np.median(confidences)*100:.2f}%)')
            print(f'  æ¨™æº–å·®: {np.std(confidences):.4f}')
            
            # ä¿¡å¿ƒåº¦åˆ†ä½ˆç™¾åˆ†æ¯”
            print(f'\nğŸ“ˆ ä¿¡å¿ƒåº¦åˆ†ä½ˆï¼š')
            bins = [0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 1.00]
            for i, bin_val in enumerate(bins):
                if i == 0:
                    count = np.sum(confidences < bin_val)
                    pct = count / len(confidences) * 100
                    print(f'  < {bin_val:.0%}: {count:6d} ({pct:5.1f}%)')
                else:
                    count = np.sum((confidences >= bins[i-1]) & (confidences < bin_val))
                    pct = count / len(confidences) * 100
                    print(f'  {bins[i-1]:.0%} - {bin_val:.0%}: {count:6d} ({pct:5.1f}%)')
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å•é¡Œ
            count_100_pct = np.sum(confidences >= 0.99)
            pct_100 = count_100_pct / len(confidences) * 100
            
            if pct_100 > 50:
                print(f'\nâš ï¸ è­¦å‘Šï¼šæœ‰ {pct_100:.1f}% çš„é æ¸¬ä¿¡å¿ƒåº¦ >= 99%')
                print(f'   é€™å¯èƒ½è¡¨ç¤ºæ¨¡å‹éåº¦æ“¬åˆæˆ–æ±ºç­–é‚Šç•Œå¤ªæ¥µç«¯')
            elif pct_100 > 10:
                print(f'\nğŸ”” æ³¨æ„ï¼šæœ‰ {pct_100:.1f}% çš„é æ¸¬ä¿¡å¿ƒåº¦ >= 99%')
                print(f'   æ‡‰è©²ç›£æ§æ¨¡å‹è¡¨ç¾')
            else:
                print(f'\nâœ… æ­£å¸¸ï¼šä¿¡å¿ƒåº¦åˆ†å¸ƒåˆç†')
            
            # åˆ†æå„é¡åˆ¥çš„é æ¸¬åˆ†å¸ƒ
            print(f'\nğŸ“Š å„é¡åˆ¥é æ¸¬åˆ†å¸ƒï¼š')
            predictions = model.predict(X_scaled)
            label_map = {0: 'ä¸‹è»Œæ”¯æ’', 1: 'ä¸­è»¸ä¸­ç«‹', 2: 'ä¸Šè»Œé˜»åŠ›'}
            
            for class_idx in range(3):
                count = np.sum(predictions == class_idx)
                pct = count / len(predictions) * 100
                avg_conf = np.mean(confidences[predictions == class_idx]) if count > 0 else 0
                print(f'  {label_map[class_idx]}: {count:6d} ({pct:5.1f}%) - å¹³å‡ä¿¡å¿ƒåº¦: {avg_conf:.2%}')
        
        except Exception as e:
            print(f'âŒ åˆ†æå¤±æ•—: {e}')
            import traceback
            traceback.print_exc()
    
    def analyze_all_symbols(self):
        """
        åˆ†ææ‰€æœ‰å¹£ç¨®çš„é æ¸¬ä¿¡å¿ƒåº¦
        """
        print('\nğŸš€ é–‹å§‹åˆ†ææ‰€æœ‰æ¨¡å‹çš„é æ¸¬ä¿¡å¿ƒåº¦...')
        
        for symbol in self.loader.symbols:
            for timeframe in self.loader.timeframes:
                self.analyze_symbol(symbol, timeframe)


if __name__ == '__main__':
    analyzer = PredictionAnalyzer()
    
    # åˆ†æå–®å€‹å¹£ç¨®
    analyzer.analyze_symbol('BTCUSDT', '15m')
    
    # åˆ†ææ‰€æœ‰å¹£ç¨®
    # analyzer.analyze_all_symbols()
