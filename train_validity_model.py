import pandas as pd
import numpy as np
import joblib
from pathlib import Path
from typing import Dict, Tuple
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score
)
import ccxt
from datetime import datetime, timedelta
import logging
import time

from data_loader import CryptoDataLoader
from validity_label_generator import ValidityLabelGenerator
from validity_features import ValidityFeatures

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ValidityModelTrainer:
    """
    è»Œé“æœ‰æ•ˆæ€§æ¨¡å‹è¨“ç·´å™¨
    """
    
    def __init__(self, models_dir='models'):
        self.models_dir = Path(models_dir)
        self.validity_models_dir = self.models_dir / 'validity_models'
        self.validity_models_dir.mkdir(parents=True, exist_ok=True)
        
        self.loader = CryptoDataLoader()
        self.label_gen = ValidityLabelGenerator(
            lookahead=10,
            min_bounce_pct=0.3,
            momentum_decay_thresh=0.15
        )
        self.feature_extractor = ValidityFeatures(lookahead=10)
    
    def train_symbol_validity_model(self, 
                                   symbol: str, 
                                   timeframe: str = '15m',
                                   test_size: float = 0.2) -> Dict:
        """
        è¨“ç·´å–®ä¸€å¹£ç¨®çš„æœ‰æ•ˆæ€§æ¨¡å‹
        """
        print(f'\n{"-"*60}')
        print(f'è¨“ç·´æœ‰æ•ˆæ€§æ¨¡å‹: {symbol} {timeframe}')
        print(f'{"-"*60}')
        
        try:
            # 1. ä¸‹è¼‰æ•¸æ“š
            print(f'\nâœ… æ­£åœ¨ä¸‹è¼‰ {symbol} {timeframe} æ•¸æ“š...')
            df = self.loader.download_symbol_data(symbol, timeframe)
            if df is None or len(df) < 200:
                print(f'  âŒ æ•¸æ“šä¸è¶³')
                return None
            
            print(f'  âœ… å·²ä¸‹è¼‰ {len(df)} æ ¹ K æ£’')
            
            # 2. ç”Ÿæˆæœ‰æ•ˆæ€§æ¨™ç±¤
            print(f'\nâœ… ç”Ÿæˆæœ‰æ•ˆæ€§æ¨™ç±¤...')
            df = self.label_gen.generate_validity_labels(df, touch_range=0.02)
            
            # çµ±è¨ˆæœ‰æ•ˆæ€§
            stats = self.label_gen.get_validity_statistics(df)
            print(f'  ä¸‹è»Œæœ‰æ•ˆç‡: {stats["support_validity_rate"]*100:.1f}%')
            print(f'  ä¸Šè»Œæœ‰æ•ˆç‡: {stats["resistance_validity_rate"]*100:.1f}%')
            print(f'  æ•´é«”æœ‰æ•ˆç‡: {stats["overall_validity_rate"]*100:.1f}%')
            
            # 3. æå–ç‰¹å¾µ
            print(f'\nâœ… æå–ç‰¹å¾µ...')
            df = self.feature_extractor.extract_all_features(df)
            
            # å£å˜ç©—è®Šé‡
            df['is_valid'] = ((df['is_valid_support'] == 1) | (df['is_valid_resistance'] == 1)).astype(int)
            
            # 4. ç²—é¸ç‰¹å¾µå’Œæ¨™ç±¤
            feature_names = self.feature_extractor.get_feature_names()
            X = df[feature_names]
            y = df['is_valid']
            
            # 5. åªç²—é¸è§¸ç¢°é»çš„æ•¸æ“š
            touch_mask = df['touch'] != 0
            X_touch = X[touch_mask]
            y_touch = y[touch_mask]
            
            if len(X_touch) < 30:
                print(f'  âŒ è§¸ç¢°æ•°æ®ä¸è¶³ ({len(X_touch)} å€‹)')
                return None
            
            print(f'  âœ… è¨“ç·´æ•°æ®: {len(X_touch)} ç­†')
            print(f'    æœ‰æ•ˆ: {y_touch.sum()} ç­† ({y_touch.sum()/len(y_touch)*100:.1f}%)')
            print(f'    ç„¡æ•ˆ: {(1-y_touch).sum()} ç­† ({(1-y_touch).sum()/len(y_touch)*100:.1f}%)')
            
            # 6. é€²è¡Œè¨“ç·´/æ¸¬è©¦åˆ†å‰²
            print(f'\nâœ… åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†...')
            X_train, X_test, y_train, y_test = train_test_split(
                X_touch, y_touch, test_size=test_size, random_state=42, stratify=y_touch
            )
            
            print(f'  è¨“ç·´é›†: {len(X_train)} ç­†')
            print(f'  æ¸¬è©¦é›†: {len(X_test)} ç­†')
            
            # 7. æ­£è­‰åŒ–ç‰¹å¾µ
            print(f'\nâœ… æ­£è­‰åŒ–ç‰¹å¾µ...')
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # 8. è¨“ç·´æ¨¡å‹
            print(f'\nâœ… è¨“ç·´ XGBoost æœ‰æ•ˆæ€§æ¨¡å‹...')
            
            # è¨ˆç®—é¡åˆ¥æ¬Šé‡
            n_valid = y_train.sum()
            n_invalid = len(y_train) - n_valid
            
            model = XGBClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                scale_pos_weight=(n_invalid / max(n_valid, 1)),
                verbosity=0
            )
            
            model.fit(X_train_scaled, y_train, verbose=0)
            
            # 9. è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            print(f'\nğŸ“Š æ¨¡å‹æ€§èƒ½:')
            
            y_train_pred = model.predict(X_train_scaled)
            y_test_pred = model.predict(X_test_scaled)
            
            train_acc = accuracy_score(y_train, y_train_pred)
            test_acc = accuracy_score(y_test, y_test_pred)
            
            train_f1 = f1_score(y_train, y_train_pred)
            test_f1 = f1_score(y_test, y_test_pred)
            
            train_prec = precision_score(y_train, y_train_pred)
            test_prec = precision_score(y_test, y_test_pred)
            
            train_recall = recall_score(y_train, y_train_pred)
            test_recall = recall_score(y_test, y_test_pred)
            
            print(f'  è¨“ç·´é›†ç²¾æº–åº¦: {train_acc:.4f} ({train_acc*100:.2f}%)')
            print(f'  æ¸¬è©¦é›†ç²¾æº–åº¦: {test_acc:.4f} ({test_acc*100:.2f}%)')
            print(f'  è¨“ç·´é›† F1: {train_f1:.4f}')
            print(f'  æ¸¬è©¦é›† F1: {test_f1:.4f}')
            print(f'  è¨“ç·´é›†ç²¾ç¨†åº¦: {train_prec:.4f}')
            print(f'  æ¸¬è©¦é›†ç²¾ç¨†åº¦: {test_prec:.4f}')
            print(f'  è¨“ç·´é›†å¬å›ç‡: {train_recall:.4f}')
            print(f'  æ¸¬è©¦é›†å¬å›ç‡: {test_recall:.4f}')
            
            # æª¢æŸ¥éä¼¼åˆ
            overfit_gap = train_acc - test_acc
            print(f'\nâš ï¸  éä¼¼åˆç¢ºèª:')
            if overfit_gap < 0.05:
                print(f'  âœ… æ²’æœ‰éä¼¼åˆ (å†’ä½‹: {overfit_gap:.4f})')
            elif overfit_gap < 0.15:
                print(f'  âš ï¸  è¼•å¾®éä¼¼åˆ (å†’ä½‹: {overfit_gap:.4f})')
            else:
                print(f'  âŒ ä¸­åº¦éä¼¼åˆ (å†’ä½‹: {overfit_gap:.4f})')
            
            # 10. ä¸²æ¨ç‰¹å¾’é‡è¦æ€§
            print(f'\nğŸ“„ ç‰¹å¾µé‡è¦æ€§æ’åº (Top 10):')
            feature_importance = model.feature_importances_
            feature_imp_df = pd.DataFrame({
                'feature': feature_names,
                'importance': feature_importance
            }).sort_values('importance', ascending=False)
            
            for idx, row in feature_imp_df.head(10).iterrows():
                print(f'  {row["feature"]:30s}: {row["importance"]:.4f}')
            
            # 11. ä¸Šå­˜æ¨¡å‹
            print(f'\nâœ… ä¸Šå­˜æ¨¡å‹...')
            symbol_model_dir = self.validity_models_dir / symbol / timeframe
            symbol_model_dir.mkdir(parents=True, exist_ok=True)
            
            model_path = symbol_model_dir / 'validity_model.pkl'
            scaler_path = symbol_model_dir / 'scaler.pkl'
            feature_names_path = symbol_model_dir / 'feature_names.pkl'
            
            joblib.dump(model, model_path)
            joblib.dump(scaler, scaler_path)
            joblib.dump(feature_names, feature_names_path)
            
            print(f'  æ¨¡å‹å·²ä¸Šå­˜åˆ°: {symbol_model_dir}')
            
            # 12. å›å‚³çµæœ
            return {
                'symbol': symbol,
                'timeframe': timeframe,
                'model': model,
                'scaler': scaler,
                'feature_names': feature_names,
                'train_acc': train_acc,
                'test_acc': test_acc,
                'train_f1': train_f1,
                'test_f1': test_f1,
                'test_precision': test_prec,
                'test_recall': test_recall,
                'overfit_gap': overfit_gap,
                'feature_importance': feature_imp_df,
                'stats': stats
            }
        
        except Exception as e:
            print(f'\nâŒ è¨“ç·´å¤±æ•—: {e}')
            import traceback
            traceback.print_exc()
            return None
    
    def train_all_symbols(self, timeframe: str = '15m') -> Dict:
        """
        è¨“ç·´æ‰€æœ‰å¹£ç¨®çš„æœ‰æ•ˆæ€§æ¨¡å‹
        """
        print(f'\n\nâœ¨ é–‹å§‹è¨“ç·´æœ‰æ•ˆæ€§æ¨¡å‹ ({len(self.loader.symbols)} å€‹å¹£ç¨®)...')
        print(f'\u6642é–“æ¡†æ¶: {timeframe}')
        
        results = {}
        successful_count = 0
        failed_symbols = []
        
        start_time = time.time()
        
        for idx, symbol in enumerate(self.loader.symbols, 1):
            print(f'\n[{idx}/{len(self.loader.symbols)}] ', end='')
            result = self.train_symbol_validity_model(symbol, timeframe)
            
            if result is not None:
                results[symbol] = result
                successful_count += 1
                print(f'  âœ… æˆåŠŸ! ç²¾æº–åº¦: {result["test_acc"]*100:.2f}%')
            else:
                failed_symbols.append(symbol)
                print(f'  âŒ å¤±æ•—')
        
        elapsed_time = time.time() - start_time
        
        # ç¶œåˆçµ±è¨ˆ
        print(f'\n\n{â€œ=â€*60}')
        print(f'ğŸ† è¨“ç·´å®Œæˆï¼')
        print(f'{â€œ=â€*60}')
        print(f'\næˆåŠŸè¨“ç·´: {successful_count}/{len(self.loader.symbols)} å€‹å¹£ç¨®')
        print(f'è¨“ç·´é€±æœŸ: {elapsed_time/60:.1f} åˆ†é˜')
        
        if failed_symbols:
            print(f'\nâŒ å¤±æ•—çš„å¹£ç¨® ({len(failed_symbols)} å€‹):' )
            for symbol in failed_symbols:
                print(f'  - {symbol}')
        
        # é¡¯ç¤ºè©³é©æ€§èƒ½
        print(f'\n\nğŸ“Š ç¶œåˆæ€§èƒ½çµ±è¨ˆ:')
        if results:
            test_accs = [r['test_acc'] for r in results.values()]
            test_f1s = [r['test_f1'] for r in results.values()]
            
            print(f'  å¹³å‡æ¸¬è©¦é›†ç²¾æº–åº¦: {np.mean(test_accs)*100:.2f}%')
            print(f'  æœ€ä½³ç²¾æº–åº¦: {np.max(test_accs)*100:.2f}% ({[s for s, r in results.items() if r["test_acc"] == np.max(test_accs)][0]})')
            print(f'  æœ€å·®ç²¾æº–åº¦: {np.min(test_accs)*100:.2f}% ({[s for s, r in results.items() if r["test_acc"] == np.min(test_accs)][0]})')
            print(f'  å¹³å‡ F1 åˆ†æ•¸: {np.mean(test_f1s):.4f}')
        
        # æ€§èƒ½æ’è¡Œ
        print(f'\n\nğŸ“‘ æ€§èƒ½æ’è¡Œ (Top 10):' )
        if results:
            sorted_results = sorted(
                [(s, r['test_acc']) for s, r in results.items()],
                key=lambda x: x[1],
                reverse=True
            )
            
            for i, (symbol, acc) in enumerate(sorted_results[:10], 1):
                print(f'  {i:2d}. {symbol:10s}: {acc*100:.2f}%')
        
        return results


if __name__ == '__main__':
    trainer = ValidityModelTrainer()
    
    # ä¿®æ”¹ä¸‹ä¸€è¡Œä¾†é¸æ“‡æ˜¯è¨“ç·´å•å€‹æˆ–æ‰€æœ‰å¹£ç¨®
    
    # é¸é … 1: è¨“ç·´å–®ä¸€å¹£ç¨®
    # result = trainer.train_symbol_validity_model('BTCUSDT', '1h')
    
    # é¸é … 2: è¨“ç·´æ‰€æœ‰å¹£ç¨® (æ¨è–¦)
    results = trainer.train_all_symbols('1h')
