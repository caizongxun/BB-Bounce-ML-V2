import pandas as pd
import numpy as np
import joblib
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from data_loader import CryptoDataLoader
from label_generator import LabelGenerator
import matplotlib.pyplot as plt

class DeepDiagnosis:
    def __init__(self, models_dir='models'):
        self.models_dir = Path(models_dir)
        self.bb_models_dir = self.models_dir / 'bb_models'
        self.loader = CryptoDataLoader()
        self.generator = LabelGenerator(period=20, std_dev=2)
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¾ K ç·šæ•¸æ“šè£½ä½œç‰¹å¾µ"""
        df = df.copy()
        close_col = 'close' if 'close' in df.columns else 'Close'
        
        if 'open' not in df.columns and 'Open' in df.columns:
            df['open'] = df['Open']
            df['high'] = df['High']
            df['low'] = df['Low']
        
        df['price_to_bb_middle'] = (df[close_col] - df['bb_middle']) / df['bb_middle']
        df['dist_upper_norm'] = (df['bb_upper'] - df[close_col]) / (df['bb_upper'] - df['bb_lower'])
        df['dist_lower_norm'] = (df[close_col] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
        df = self.calculate_rsi(df)
        df['volatility'] = df['volatility'].fillna(df['volatility'].mean())
        df['returns'] = df[close_col].pct_change()
        df['returns_std'] = df['returns'].rolling(window=20).std()
        df['high_low_ratio'] = df['high'] / df['low'] - 1 if 'high' in df.columns else 0
        df['close_open_ratio'] = df[close_col] / df['open'] - 1 if 'open' in df.columns else 0
        df['sma_5'] = df[close_col].rolling(window=5).mean()
        df['sma_20'] = df[close_col].rolling(window=20).mean()
        df['sma_50'] = df[close_col].rolling(window=50).mean()
        df = df.ffill().bfill()
        return df
    
    def calculate_rsi(self, df: pd.DataFrame, period=14) -> pd.DataFrame:
        df = df.copy()
        close_col = 'close' if 'close' in df.columns else 'Close'
        delta = df[close_col].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        df['rsi'] = df['rsi'].fillna(50)
        return df
    
    def diagnose_symbol(self, symbol: str, timeframe: str):
        """
        æ·±åº¦è¨ºæ–·ç‰¹å®šå¹£ç¨®
        """
        print(f'\n{"="*70}')
        print(f'ğŸ”¬ æ·±åº¦è¨ºæ–· {symbol} {timeframe}')
        print(f'{"="*70}')
        
        try:
            # 1. åŠ è¼‰æ¨¡å‹
            model_path = self.bb_models_dir / symbol / timeframe / 'model.pkl'
            scaler_path = self.bb_models_dir / symbol / timeframe / 'scaler.pkl'
            
            if not model_path.exists():
                print(f'âŒ æ¨¡å‹ä¸å­˜åœ¨')
                return
            
            model = joblib.load(model_path)
            scaler = joblib.load(scaler_path)
            
            # 2. ä¸‹è¼‰æ•¸æ“š
            df = self.loader.download_symbol_data(symbol, timeframe)
            if df is None:
                print(f'âŒ ä¸‹è¼‰å¤±æ•—')
                return
            
            print(f'âœ… å·²åŠ è¼‰ {len(df)} æ ¹ K æ£’')
            
            # 3. ç”Ÿæˆæ¨™ç±¤å’Œç‰¹å¾µ
            df = self.generator.create_training_dataset(df, lookahead=5, touch_range=0.02)
            df = self.create_features(df)
            
            # 4. åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
            feature_cols = [
                'price_to_bb_middle', 'dist_upper_norm', 'dist_lower_norm',
                'bb_width', 'rsi', 'volatility', 'returns_std',
                'high_low_ratio', 'close_open_ratio',
                'sma_5', 'sma_20', 'sma_50'
            ]
            
            X = df[feature_cols].ffill().bfill()
            y = df['bb_touch_label']
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # 5. åˆ†æè¨“ç·´ vs æ¸¬è©¦æ•¸æ“šåˆ†å¸ƒ
            print(f'\nğŸ“Š æ•¸æ“šåˆ†å¸ƒåˆ†æï¼š')
            print(f'  è¨“ç·´é›†æ¨™ç±¤åˆ†å¸ƒï¼š')
            for label in [-1, 0, 1]:
                count = np.sum(y_train == label)
                pct = count / len(y_train) * 100
                print(f'    {label:2d}: {count:6d} ({pct:5.1f}%)')
            
            print(f'  æ¸¬è©¦é›†æ¨™ç±¤åˆ†å¸ƒï¼š')
            for label in [-1, 0, 1]:
                count = np.sum(y_test == label)
                pct = count / len(y_test) * 100
                print(f'    {label:2d}: {count:6d} ({pct:5.1f}%)')
            
            # 6. åˆ†æç‰¹å¾µçµ±è¨ˆ
            print(f'\nğŸ“ˆ ç‰¹å¾µçµ±è¨ˆï¼ˆè¨“ç·´é›†ï¼‰ï¼š')
            print(f'  ç‰¹å¾µåç¨±                  | æœ€å°å€¼   | æœ€å¤§å€¼   | å¹³å‡å€¼   | æ¨™æº–å·®')
            print(f'  {"â”€"*70}')
            for col in feature_cols:
                min_val = X_train[col].min()
                max_val = X_train[col].max()
                mean_val = X_train[col].mean()
                std_val = X_train[col].std()
                print(f'  {col:25s} | {min_val:8.4f} | {max_val:8.4f} | {mean_val:8.4f} | {std_val:8.4f}')
            
            # 7. è¨ˆç®—æ¨¡å‹åœ¨è¨“ç·´é›†çš„æ€§èƒ½
            print(f'\nğŸ¯ è¨“ç·´é›†æ€§èƒ½ï¼š')
            X_train_scaled = scaler.fit_transform(X_train)
            
            # è½‰æ›æ¨™ç±¤
            label_map = {-1: 0, 0: 1, 1: 2}
            y_train_mapped = np.array([label_map[int(label)] for label in y_train])
            
            train_proba = model.predict_proba(X_train_scaled)
            train_predictions = model.predict(X_train_scaled)
            train_confidences = np.max(train_proba, axis=1)
            
            train_accuracy = np.mean(train_predictions == y_train_mapped)
            
            print(f'  è¨“ç·´é›†ç²¾æº–åº¦: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)')
            print(f'  è¨“ç·´é›†å¹³å‡ä¿¡å¿ƒåº¦: {np.mean(train_confidences):.4f} ({np.mean(train_confidences)*100:.2f}%)')
            print(f'  è¨“ç·´é›†æœ€å°ä¿¡å¿ƒåº¦: {np.min(train_confidences):.4f} ({np.min(train_confidences)*100:.2f}%)')
            print(f'  è¨“ç·´é›† >= 99% çš„æ¯”ä¾‹: {np.mean(train_confidences >= 0.99)*100:.2f}%')
            
            # 8. è¨ˆç®—æ¨¡å‹åœ¨æ¸¬è©¦é›†çš„æ€§èƒ½
            print(f'\nğŸ” æ¸¬è©¦é›†æ€§èƒ½ï¼š')
            X_test_scaled = scaler.transform(X_test)
            y_test_mapped = np.array([label_map[int(label)] for label in y_test])
            
            test_proba = model.predict_proba(X_test_scaled)
            test_predictions = model.predict(X_test_scaled)
            test_confidences = np.max(test_proba, axis=1)
            
            test_accuracy = np.mean(test_predictions == y_test_mapped)
            
            print(f'  æ¸¬è©¦é›†ç²¾æº–åº¦: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)')
            print(f'  æ¸¬è©¦é›†å¹³å‡ä¿¡å¿ƒåº¦: {np.mean(test_confidences):.4f} ({np.mean(test_confidences)*100:.2f}%)')
            print(f'  æ¸¬è©¦é›†æœ€å°ä¿¡å¿ƒåº¦: {np.min(test_confidences):.4f} ({np.min(test_confidences)*100:.2f}%)')
            print(f'  æ¸¬è©¦é›† >= 99% çš„æ¯”ä¾‹: {np.mean(test_confidences >= 0.99)*100:.2f}%')
            
            # 9. éæ“¬åˆæª¢æŸ¥
            print(f'\nâš ï¸ éæ“¬åˆæª¢æŸ¥ï¼š')
            acc_gap = train_accuracy - test_accuracy
            conf_gap = np.mean(train_confidences) - np.mean(test_confidences)
            
            print(f'  ç²¾æº–åº¦å·®: {acc_gap:.4f} ({acc_gap*100:.2f}%)')
            print(f'  ä¿¡å¿ƒåº¦å·®: {conf_gap:.4f} ({conf_gap*100:.2f}%)')
            
            if acc_gap < 0.01 and conf_gap < 0.01:
                print(f'  âœ… æ²’æœ‰éæ“¬åˆè·¡è±¡')
            elif acc_gap < 0.05:
                print(f'  âš ï¸  è¼•å¾®éæ“¬åˆï¼Œä½†å¯æ¥å—')
            else:
                print(f'  âŒ ä¸­ç­‰éæ“¬åˆ')
            
            # 10. æ±ºç­–é‚Šç•Œåˆ†æ
            print(f'\nğŸ”¬ æ±ºç­–é‚Šç•Œåˆ†æï¼š')
            print(f'  æª¢æŸ¥æ˜¯å¦æŸå€‹ç‰¹å¾µä¸»å°æ±ºç­–...')
            
            # è¨ˆç®—ç‰¹å¾µé‡è¦æ€§
            feature_importance = model.feature_importances_
            sorted_idx = np.argsort(feature_importance)[::-1]
            
            print(f'\n  å‰ 5 å€‹æœ€é‡è¦çš„ç‰¹å¾µï¼š')
            for i in range(min(5, len(feature_cols))):
                idx = sorted_idx[i]
                importance = feature_importance[idx]
                print(f'    {i+1}. {feature_cols[idx]:25s}: {importance:.4f}')
            
            top_importance_sum = np.sum(feature_importance[sorted_idx[:3]]) / np.sum(feature_importance)
            print(f'\n  å‰ 3 å€‹ç‰¹å¾µä½”æ¯”: {top_importance_sum*100:.1f}%')
            
            if top_importance_sum > 0.7:
                print(f'  âš ï¸  è­¦å‘Š: æ¨¡å‹æ±ºç­–éåº¦ä¾è³´å°‘æ•¸ç‰¹å¾µ')
            else:
                print(f'  âœ… æ¨¡å‹ä½¿ç”¨å¤šå€‹ç‰¹å¾µé€²è¡Œæ±ºç­–')
        
        except Exception as e:
            print(f'âŒ è¨ºæ–·å¤±æ•—: {e}')
            import traceback
            traceback.print_exc()

if __name__ == '__main__':
    diagnosis = DeepDiagnosis()
    
    # è¨ºæ–·å–®å€‹å¹£ç¨®
    diagnosis.diagnose_symbol('BTCUSDT', '15m')
    diagnosis.diagnose_symbol('ETHUSDT', '1h')
